{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook contains solutions to the exercises from Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Number of parameters in feed forward and attention modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:06:32.491513Z",
     "start_time": "2025-04-24T16:06:32.466196Z"
    }
   },
   "source": [
    "import torch\n",
    "from gpt import TransformerBlock\n",
    "import os\n",
    "import sys\n",
    "import gpt\n",
    "\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/PycharmProjects/ArtLLM/ch04\"))\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "ff_params = sum(p.numel() for p in block.ff.parameters())\n",
    "attn_params = sum(p.numel() for p in block.att.parameters())\n",
    "\n",
    "print(f\"Feedforward parameters: {ff_params:,}\")\n",
    "print(f\"Attention parameters: {attn_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward parameters: 4,722,432\n",
      "Attention parameters: 2,360,064\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:05:00.414413Z",
     "start_time": "2025-04-24T16:05:00.411429Z"
    }
   },
   "cell_type": "code",
   "source": "print(gpt.__file__)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Msi/PycharmProjects/ArtLLM/ch04/gpt.py\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Exercise 4.2: Initializing Larger GPT Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:06:44.812298Z",
     "start_time": "2025-04-24T16:06:36.156316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt import GPTModel\n",
    "\n",
    "def print_model_size(name, cfg):\n",
    "    model = GPTModel(cfg)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name} → Total parameters: {total_params:,}\")\n",
    "\n",
    "# GPT-2 Medium\n",
    "print_model_size(\"GPT-2 Medium\", {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1024,\n",
    "    \"n_heads\": 16,\n",
    "    \"n_layers\": 24,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "})\n",
    "\n",
    "# GPT-2 Large\n",
    "print_model_size(\"GPT-2 Large\", {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1280,\n",
    "    \"n_heads\": 20,\n",
    "    \"n_layers\": 36,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "})\n",
    "\n",
    "# GPT-2 XL\n",
    "print_model_size(\"GPT-2 XL\", {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1600,\n",
    "    \"n_heads\": 25,\n",
    "    \"n_layers\": 48,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Medium → Total parameters: 406,212,608\n",
      "GPT-2 Large → Total parameters: 838,220,800\n",
      "GPT-2 XL → Total parameters: 1,637,792,000\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 4.3: Using Separate Dropout Parameters"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:07:07.523576Z",
     "start_time": "2025-04-24T16:07:06.898254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gptmodel import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate_attn\": 0.1,\n",
    "    \"drop_rate_shortcut\": 0.1,\n",
    "    \"drop_rate_emb\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "print(\"Model successfully initialized with:\")\n",
    "print(\"- Attention dropout:\", GPT_CONFIG_124M[\"drop_rate_attn\"])\n",
    "print(\"- Shortcut dropout:\", GPT_CONFIG_124M[\"drop_rate_shortcut\"])\n",
    "print(\"- Embedding dropout:\", GPT_CONFIG_124M[\"drop_rate_emb\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully initialized with:\n",
      "- Attention dropout: 0.1\n",
      "- Shortcut dropout: 0.1\n",
      "- Embedding dropout: 0.1\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
