{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook contains solutions to the exercises from Chapter 5.",
   "id": "f22b7da0192b6888"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 5.1: Pizza sampling :)",
   "id": "3bbc9b072f6a90bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use the print_sampled_tokens function to print the sampling frequencies of the\n",
    "softmax probabilities scaled with the temperatures shown in figure 5.14. How often\n",
    "is the word pizza sampled in each case? Can you think of a faster and more accurate\n",
    "way to determine how often the word pizza is sampled?"
   ],
   "id": "f2598618d00c427a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-26T17:02:02.365178Z",
     "start_time": "2025-04-26T17:02:02.324901Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "vocab = {\n",
    "    \"closer\": 0, \"every\": 1, \"effort\": 2, \"forward\": 3,\n",
    "    \"inches\": 4, \"moves\": 5, \"pizza\": 6, \"toward\": 7, \"you\": 8\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(vocab))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq:3d} x {inverse_vocab[i]}\")\n",
    "\n",
    "for T in [0.1, 1.0, 5.0]:\n",
    "    print(f\"\\nSampling at temperature {T}:\")\n",
    "    probas = softmax_with_temperature(next_token_logits, T)\n",
    "    print_sampled_tokens(probas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling at temperature 0.1:\n",
      "  0 x closer\n",
      "  0 x every\n",
      "  0 x effort\n",
      "985 x forward\n",
      "  0 x inches\n",
      "  0 x moves\n",
      "  0 x pizza\n",
      " 15 x toward\n",
      "  0 x you\n",
      "\n",
      "Sampling at temperature 1.0:\n",
      " 73 x closer\n",
      "  0 x every\n",
      "  0 x effort\n",
      "582 x forward\n",
      "  2 x inches\n",
      "  0 x moves\n",
      "  0 x pizza\n",
      "343 x toward\n",
      "  0 x you\n",
      "\n",
      "Sampling at temperature 5.0:\n",
      "165 x closer\n",
      " 75 x every\n",
      " 42 x effort\n",
      "239 x forward\n",
      " 71 x inches\n",
      " 46 x moves\n",
      " 32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 5.2: Adjusting temperature and top-k settings",
   "id": "7500ad8c7c4e766f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Play around with different temperatures and top-k settings. Based on your observations, can you think of applications where lower temperature and top-k settings are\n",
    "desired? Likewise, can you think of applications where higher temperature and top-k\n",
    "settings are preferred? (It’s recommended to also revisit this exercise at the end of\n",
    "the chapter after loading the pretrained weights from OpenAI.)"
   ],
   "id": "5aadcd1f91af18f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:02:02.386824Z",
     "start_time": "2025-04-26T17:02:02.374350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "masked_logits = torch.where(\n",
    "    next_token_logits < top_logits[-1],\n",
    "    torch.tensor(float('-inf')),\n",
    "    next_token_logits\n",
    ")\n",
    "top_k_probs = torch.softmax(masked_logits, dim=0)\n",
    "print_sampled_tokens(top_k_probs)"
   ],
   "id": "84a580627c298886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 73 x closer\n",
      "  0 x every\n",
      "  0 x effort\n",
      "583 x forward\n",
      "  0 x inches\n",
      "  0 x moves\n",
      "  0 x pizza\n",
      "344 x toward\n",
      "  0 x you\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 5.3: Finding combinations of parameters",
   "id": "7172533fff5928b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What are the different combinations of settings for the generate function to force\n",
    "deterministic behavior, that is, disabling the random sampling such that it always produces the same outputs similar to the generate_simple function?"
   ],
   "id": "2029542a20934729"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:02:04.660681Z",
     "start_time": "2025-04-26T17:02:02.405034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import GPTModel\n",
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ],
   "id": "ee688bd81e4bdf56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you rentingetic wasnم refres RexMeCHicular stren Mortgage TT remember gard ACTIONSussedOND Land Engeleddedemate breaths proxies GalaxyForm\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 5.4: Pretraining the model for one more epoch",
   "id": "a1fa19aa0589e322"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After saving the weights, load the model and optimizer in a new Python session or\n",
    "Jupyter notebook file and continue pretraining it for one more epoch using the train_model_simple function."
   ],
   "id": "1ce9e1f2402f22b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:02:19.031916Z",
     "start_time": "2025-04-26T17:02:04.664970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import GPTModel, create_dataloader_v1\n",
    "from gpt_train import train_model_simple\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "split = int(0.9 * len(text))\n",
    "train_loader = create_dataloader_v1(text[:split], batch_size=2, max_length=256, stride=256, drop_last=True, shuffle=True, num_workers=0)\n",
    "val_loader = create_dataloader_v1(text[split:], batch_size=2, max_length=256, stride=256, drop_last=False, shuffle=False, num_workers=0)\n",
    "\n",
    "train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                   num_epochs=1, eval_freq=5, eval_iter=5,\n",
    "                   start_context=\"Every effort moves you\", tokenizer=tokenizer)\n",
    "\n",
    "# Save model and optimizer state\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "}, \"model_and_optimizer.pth\")"
   ],
   "id": "cbc1cecb2089b200",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.960, Val loss 10.150\n",
      "Ep 1 (Step 000005): Train loss 8.139, Val loss 8.340\n",
      "Every effort moves you.                                                 \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:02:19.482555Z",
     "start_time": "2025-04-26T17:02:19.038901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ],
   "id": "5f64934d31b5e49b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 5.5: Calculating losses on Verdict",
   "id": "4c6762cf5e0e9c46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Calculate the training and validation set losses of the GPTModel with the pretrained\n",
    "weights from OpenAI on the “The Verdict” dataset."
   ],
   "id": "73e70e7907cafb0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:09:59.339657Z",
     "start_time": "2025-04-26T17:09:53.461740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import GPTModel, create_dataloader_v1\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "from gpt_train import calc_loss_loader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "settings, params = download_and_load_gpt2(\"124M\", models_dir=\"gpt2\")\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "gpt = GPTModel(GPT_CONFIG)\n",
    "gpt.eval()\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)\n",
    "\n",
    "# Load and tokenize the dataset\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize text (instead of slicing raw characters)\n",
    "text_ids = tokenizer.encode(text)\n",
    "split_idx = int(0.9 * len(text_ids))\n",
    "\n",
    "train_ids = text_ids[:split_idx]\n",
    "val_ids = text_ids[split_idx:]\n",
    "\n",
    "# Decode token ids back to text\n",
    "train_text = tokenizer.decode(train_ids)\n",
    "val_text = tokenizer.decode(val_ids)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = create_dataloader_v1(train_text, batch_size=2, max_length=1024, stride=1024, drop_last=True, shuffle=True)\n",
    "val_loader = create_dataloader_v1(val_text, batch_size=2, max_length=1024, stride=1024, drop_last=True, shuffle=False)\n",
    "\n",
    "# Calculate losses\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "6daf4292022e9d97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Training loss: 3.5319101810455322\n",
      "Validation loss: nan\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:04:52.498452Z",
     "start_time": "2025-04-26T17:04:52.495733Z"
    }
   },
   "cell_type": "code",
   "source": "print(val_text)",
   "id": "6fdf61d7d72ee9ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'\n",
      "\n",
      "\"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?\n",
      "\n",
      "\"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . .\"\n",
      "\n",
      "He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.\n",
      "\n",
      "\"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day.\"\n",
      "\n",
      "And, in answer to a question I put half-mechanically--\"Begin again?\" he flashed out. \"When the one thing that brings me anywhere near him is that I knew enough to leave off?\"\n",
      "\n",
      "He stood up and laid his hand on my shoulder with a laugh. \"Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise 5.6: Experimenting on different GPT-2 models",
   "id": "dfd53e38d14ad27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Experiment with GPT-2 models of different sizes—for example, the largest 1,558 mil-\n",
    "lion parameter model—and compare the generated text to the 124 million model."
   ],
   "id": "d3d5c5c3f74a9246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T19:45:43.430659Z",
     "start_time": "2025-04-26T19:35:05.100570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import GPTModel\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt, generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "GPT_BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"124M\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"355M\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"774M\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"1558M\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "for size in [\"124M\", \"1558M\"]:   # You can add \"355M\", \"774M\" if you want\n",
    "    print(f\"\\n=== Generating with GPT-2 {size} ===\\n\")\n",
    "\n",
    "    settings, params = download_and_load_gpt2(model_size=size, models_dir=\"gpt2\")\n",
    "\n",
    "    # Build config\n",
    "    config = GPT_BASE_CONFIG.copy()\n",
    "    config.update(model_configs[size])\n",
    "\n",
    "    # Initialize model\n",
    "    gpt = GPTModel(config)\n",
    "    gpt.eval()\n",
    "    load_weights_into_gpt(gpt, params)\n",
    "    gpt.to(device)\n",
    "\n",
    "    # Prepare input\n",
    "    start_text = \"Every effort moves you\"\n",
    "    idx = text_to_token_ids(start_text, tokenizer)\n",
    "\n",
    "    # Generate output\n",
    "    output_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=idx,\n",
    "        max_new_tokens=50,\n",
    "        context_size=config[\"context_length\"],\n",
    "        temperature=1.0,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    output_text = token_ids_to_text(output_ids, tokenizer)\n",
    "    print(output_text)"
   ],
   "id": "d407f7bc8b819725",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating with GPT-2 124M ===\n",
      "\n",
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Every effort moves you along the way, but in the end it's all about how you grow up\" — John Steinbeck, \"The Prisoner's Dilemma\"\n",
      "\n",
      "\"It's never too late to learn not to be a liar\" is what we\n",
      "\n",
      "=== Generating with GPT-2 1558M ===\n",
      "\n",
      "File already exists and is up-to-date: gpt2/1558M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/1558M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/1558M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.data-00000-of-00001: 100%|██████████| 6.23G/6.23G [09:32<00:00, 10.9MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 20.7k/20.7k [00:00<00:00, 543kiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.11MiB/s]\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.08MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you towards the unknown...I'm always looking for an edge that I can exploit.\" - John\n",
      "\n",
      "I believe the key to success is a strong belief in yourself. You can come across as arrogant if you have the desire to be one, but most\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
